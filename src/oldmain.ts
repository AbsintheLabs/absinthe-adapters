// // main.js
// // This is the main executable of the squid indexer.
// import fs from 'fs';
// // EvmBatchProcessor is the class responsible for data retrieval and processing.
// import { EvmBatchProcessor } from '@subsquid/evm-processor'
// // TypeormDatabase is the class responsible for data storage.
// import { TypeormDatabase } from '@subsquid/typeorm-store'
// import { Database, LocalDest } from '@subsquid/file-store'
// import { Balances } from './tables';
// import Bottleneck from 'bottleneck';

// // usdcAbi is a utility module generated from the JSON ABI of the USDC contract.
// // It contains methods for event decoding, direct RPC queries and some useful
// // constants.
// import * as usdcAbi from './abi/usdc'
// // UsdcTransfer is a TypeORM model class of the usdc_transfer Postgres table.
// // It is autogenerated from schema.graphql.
// import { UsdcTransfer } from './model'
// import { Tokens } from './tables';

// const USDC_CONTRACT_ADDRESS =
//   // '0xb5b1b659da79a2507c27aad509f15b4874edc0cc'
//   // '0x04C618CDbc1D59142dFEB4B9864835A06983EC2d'
//   // '0xa3F4341C3fEf5963AB04135D2014AC7d68222E19'
//   '0xa26b04b41162b0d7c2e1e2f9a33b752e28304a49';

// const limiter = new Bottleneck({
//   maxConcurrent: 1,
//   minTime: 110, // space out the calls and slightly more to not hit against the window
//   // reservoir: 0, // Start with 10 tokens
//   // reservoirRefreshAmount: 10, // Refill 10 tokens per second
//   // reservoirRefreshInterval: 1000, // Refresh every .1 seconds
// });

// // First we configure data retrieval.
// const processor = new EvmBatchProcessor()
//   // Find all gateways here for your network: https://docs.sqd.ai/subsquid-network/reference/networks/
//   .setGateway('https://v2.archive.subsquid.io/network/ethereum-mainnet')
//   // You can use RPCs to get extra data during indexing
//   // If a gateway is not available for your network, you must supply an RPC endpoint
//   .setRpcEndpoint('https://eth.llamarpc.com')
//   // The processor needs to know how many newest blocks it should mark as "hot".
//   // If it detects a blockchain fork, it will roll back any changes to the
//   // database made due to orphaned blocks, then re-run the processing for the
//   // main chain blocks.
//   .setFinalityConfirmation(75)
//   // .addXXX() methods request data items.
//   // .addTransaction(), .addTrace(), .addStateDiff() are also available
//   .addLog({
//     // range: { from: 21557766 },
//     // You should supply the "from" to be the contract creation block
//     range: { from: 21557766, to: 21615680 },
//     address: [USDC_CONTRACT_ADDRESS],
//     topic0: [usdcAbi.events.Transfer.topic],
//   })
//   // .setFields() is for choosing data fields for the selected data items.
//   // Here we're requesting hashes of parent transaction for all event logs.
//   .setFields({
//     log: {
//       transactionHash: true,
//     },
//   })

// // const db = new Database({ tables: { Tokens, Balances }, dest: new LocalDest('./data'), chunkSizeMb: 1 });
// const db = new TypeormDatabase({ supportHotBlocks: false })
// let nextHourlyFlush = 0;
// let lastInterpolatedTs: number | null = null;
// const addressMap = new Map<string, { balance: bigint, windowStartTs: number }>();

// type AddressBalance = { address: string, balance: bigint, windowStartTs: number, windowEndTs: number };
// const balances: AddressBalance[] = [];
// const msPerHour = 3600 * 1000;

// // Helper function to convert BigInt values to strings for JSON serialization
// const convertBigIntToString = (obj: any): any => {
//   if (obj === null || obj === undefined) {
//     return obj;
//   }

//   if (typeof obj === 'bigint') {
//     return obj.toString();
//   }

//   if (Array.isArray(obj)) {
//     return obj.map(convertBigIntToString);
//   }

//   if (typeof obj === 'object') {
//     const result: any = {};
//     for (const key in obj) {
//       result[key] = convertBigIntToString(obj[key]);
//     }
//     return result;
//   }

//   return obj;
// };

// // Retry function with exponential backoff
// async function fetchWithRetry(
//   url: string,
//   options: RequestInit,
//   maxRetries = 5,
//   initialBackoffMs = 1000
// ): Promise<Response> {
//   let retries = 0;
//   let backoffMs = initialBackoffMs;

//   while (true) {
//     try {
//       const response = await fetch(url, options);

//       if (response.status >= 200 && response.status < 300) {
//         return response;
//       }

//       console.log(`Request failed with status: ${response.status}`);

//       if (retries >= maxRetries) {
//         console.log(`Maximum retries (${maxRetries}) reached. Failing.`);
//         return response; // Return the failed response after max retries
//       }
//     } catch (error) {
//       console.error(`Network error during fetch: ${error instanceof Error ? error.message : String(error)}`);

//       if (retries >= maxRetries) {
//         console.log(`Maximum retries (${maxRetries}) reached. Failing.`);
//         throw error; // Re-throw the error after max retries
//       }
//     }

//     // Exponential backoff with jitter
//     const jitter = Math.random() * 0.3 + 0.85; // Random between 0.85 and 1.15
//     const waitTime = Math.floor(backoffMs * jitter);
//     console.log(`Retry ${retries + 1}/${maxRetries}. Waiting ${waitTime}ms before next attempt...`);

//     // Wait for backoff period
//     await new Promise(resolve => setTimeout(resolve, waitTime));

//     // Increase backoff for next try (exponential)
//     backoffMs *= 2;
//     retries++;
//   }
// }

// // Function to send balance data to API with blocking retries
// async function sendBalancesToApi(balances: AddressBalance[]): Promise<void> {
//   if (balances.length === 0) {
//     return;
//   }

//   console.log(`Sending ${balances.length} balance records to API...`);

//   let success = false;
//   let retryCount = 0;
//   const MAX_RETRIES = 10; // Maximum number of retry attempts
//   let backoffMs = 1000; // Start with 1 second

//   // Create a copy of the balances to ensure data isn't lost if there's an error
//   const balancesCopy = [...balances];

//   while (!success && retryCount < MAX_RETRIES) {
//     try {
//       const response = await limiter.schedule(() => fetchWithRetry('http://localhost:3000/api/log', {
//         method: 'POST',
//         headers: {
//           'Content-Type': 'application/json',
//           'x-api-key': 'api_key_2',
//         },
//         body: JSON.stringify(convertBigIntToString({ balances: balancesCopy })),
//       }));

//       if (response.status === 200) {
//         console.log("Successfully sent balance data to API");
//         success = true;
//       } else {
//         retryCount++;
//         console.log(`API returned error status ${response.status}. Retry ${retryCount}/${MAX_RETRIES}`);

//         if (retryCount >= MAX_RETRIES) {
//           console.error("Failed to send balances to API after maximum retries");
//           throw new Error(`Failed to send balances after ${MAX_RETRIES} attempts`);
//         }

//         // Wait before next retry with exponential backoff
//         const waitTime = backoffMs * (1 + Math.random() * 0.2); // Add some jitter
//         console.log(`Waiting ${Math.round(waitTime)}ms before retry ${retryCount + 1}...`);
//         await new Promise(resolve => setTimeout(resolve, waitTime));
//         backoffMs *= 2; // Double the backoff time for next retry
//       }
//     } catch (error) {
//       retryCount++;
//       console.error("Error sending balances to API:", error);

//       if (retryCount >= MAX_RETRIES) {
//         console.error("Failed to send balances to API after maximum retries");
//         throw new Error(`Failed to send balances after ${MAX_RETRIES} attempts: ${error}`);
//       }

//       // Wait before next retry with exponential backoff
//       const waitTime = backoffMs * (1 + Math.random() * 0.2); // Add some jitter
//       console.log(`Waiting ${Math.round(waitTime)}ms before retry ${retryCount + 1}...`);
//       await new Promise(resolve => setTimeout(resolve, waitTime));
//       backoffMs *= 2; // Double the backoff time for next retry
//     }
//   }
// }

// // The processor.run() call executes the data processing. Its second argument is
// // the handler function that is executed once on each batch of data. Processor
// // object provides the data via "ctx.blocks". However, the handler can contain
// // arbitrary TypeScript code, so it's OK to bring in extra data from IPFS,
// // direct RPC calls, external APIs etc.
// processor.run(db, async (ctx) => {
//   // Making the container to hold that which will become the rows of the
//   // usdc_transfer database table while processing the batch. We'll insert them
//   // all at once at the end, massively saving IO bandwidth.
//   const transfers: UsdcTransfer[] = []

//   try {
//     for (let block of ctx.blocks) {
//       for (let log of block.logs) {
//         // NOTE: Step 1: Emit events on transfer
//         if (log.address === USDC_CONTRACT_ADDRESS &&
//           log.topics[0] === usdcAbi.events.Transfer.topic) {
//           // SQD decode is 20x faster than ethers
//           let { from, to, value } = usdcAbi.events.Transfer.decode(log)
//           const prevEntryFrom = addressMap.get(from);
//           const prevEntryTo = addressMap.get(to);
//           const prevBalanceFrom = prevEntryFrom ? prevEntryFrom.balance : 0n;
//           const prevWindowStartTsFrom = prevEntryFrom ? prevEntryFrom.windowStartTs : block.header.timestamp;
//           const prevBalanceTo = prevEntryTo ? prevEntryTo.balance : 0n;
//           const prevWindowStartTsTo = prevEntryTo ? prevEntryTo.windowStartTs : block.header.timestamp;
//           // we only emit a balance change if there is a previous entry
//           if (prevEntryFrom) {
//             balances.push({ address: from, balance: prevBalanceFrom, windowStartTs: prevWindowStartTsFrom, windowEndTs: block.header.timestamp });
//           }
//           if (prevEntryTo) {
//             balances.push({ address: to, balance: prevBalanceTo, windowStartTs: prevWindowStartTsTo, windowEndTs: block.header.timestamp });
//           }
//           // update the balance in the map with the new balance and window start time
//           addressMap.set(to, { balance: prevBalanceTo + value, windowStartTs: block.header.timestamp });
//           addressMap.set(from, { balance: prevBalanceFrom - value, windowStartTs: block.header.timestamp });

//           transfers.push(new UsdcTransfer({
//             id: log.id,
//             block: block.header.height,
//             from,
//             to,
//             value,
//             txnHash: log.transactionHash
//           }))
//         }
//       }

//       // NOTE: Step 2: Interpolate balances based on block range
//       // flush balances after the logs
//       const currentTs = block.header.timestamp;
//       if (!lastInterpolatedTs) {
//         lastInterpolatedTs = currentTs;
//       }
//       // if (currentTs >= nextHourlyFlush) {
//       while (lastInterpolatedTs! + msPerHour <= currentTs) {
//         const nextBoundaryTs: number = lastInterpolatedTs! + msPerHour;
//         console.log("Hourly flushing at " + (lastInterpolatedTs! + msPerHour));
//         console.log("currentTs " + currentTs);
//         // ... do hourly flush...
//         for (let [address, data] of addressMap.entries()) {
//           const oldStart = data.windowStartTs;
//           if (data.balance > 0 && oldStart < nextBoundaryTs) {
//             balances.push({ address, balance: data.balance, windowStartTs: oldStart, windowEndTs: nextBoundaryTs });
//             addressMap.set(address, { balance: data.balance, windowStartTs: nextBoundaryTs });
//           }
//         }

//         // Write balance records to Balances table after each hourly flush
//         if (balances.length > 0) {
//           console.log(`Writing ${balances.length} balance records from hourly flush`);
//           // NOTE: removing the store to deal with this later
//           // await ctx.store.Balances.writeMany(balances.map(b => ({
//           //   address: b.address,
//           //   balance: b.balance,
//           //   windowStartTs: b.windowStartTs,
//           //   windowEndTs: b.windowEndTs,
//           // })));
//           console.log("balances", balances);

//           // Ensure API calls succeed before clearing balances
//           try {
//             await sendBalancesToApi(balances);
//             console.log("Clearing balances array after hourly flush");
//             balances.length = 0;
//           } catch (error) {
//             console.error("Failed to send balances to API, will retry in next batch:", error);
//             // Don't clear balances array - will retry in next batch
//           }
//         }

//         // set the next flush to be one hour from current block timestamp
//         // nextHourlyFlush = Math.floor(nextHourlyFlush / 3600) * 3600 + 3600;
//         // nextHourlyFlush = Math.floor(currentTs / msPerHour) * msPerHour + msPerHour;
//         lastInterpolatedTs = nextBoundaryTs;
//       }

//       if (block.header.height === 21615680) {
//         console.log("Reached final block, ensuring all balance data is written");
//         // Final block processing completed - all data should be in Parquet files
//       }
//     }

//     // todo: need to figure out how to not do this stupid casting
//     console.log(`Writing ${balances.length} balance records to Balances table`);
//     console.log("Forcing flush to ensure data is written to disk");

//     // Write the balances to the Balances table
//     if (balances.length > 0) {
//       // NOTE: removing the store to deal with this later
//       // await ctx.store.insert(Balances.values(balances.map(b => ({
//       //   address: b.address,
//       //   balance: b.balance,
//       //   windowStartTs: b.windowStartTs,
//       //   windowEndTs: b.windowEndTs,
//       // })));
//       console.log("balances", balances);

//       // Ensure API calls succeed before moving on
//       await sendBalancesToApi(balances);
//       console.log("Clearing balances array to save memory");
//       balances.length = 0;
//     } else {
//       console.log("No balance records to write");
//     }

//     // ctx.store.setForceFlush(true);
//   } catch (error) {
//     console.error("Error in processing block batch:", error);
//     // Rethrow to let the processor handle the error
//     throw error;
//   }
// })
