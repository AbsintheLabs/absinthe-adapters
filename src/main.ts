// main.js
// This is the main executable of the squid indexer.
import fs from 'fs';
// EvmBatchProcessor is the class responsible for data retrieval and processing.
import { EvmBatchProcessor } from '@subsquid/evm-processor'
// TypeormDatabase is the class responsible for data storage.
import { TypeormDatabase } from '@subsquid/typeorm-store'
// usdcAbi is a utility module generated from the JSON ABI of the USDC contract.
// It contains methods for event decoding, direct RPC queries and some useful
// constants.
import * as usdcAbi from './abi/usdc'
// UsdcTransfer is a TypeORM model class of the usdc_transfer Postgres table.
// It is autogenerated from schema.graphql.
import { UsdcTransfer } from './model'

const USDC_CONTRACT_ADDRESS =
  // '0xb5b1b659da79a2507c27aad509f15b4874edc0cc'
  // '0x04C618CDbc1D59142dFEB4B9864835A06983EC2d'
  // '0xa3F4341C3fEf5963AB04135D2014AC7d68222E19'
  '0xa26b04b41162b0d7c2e1e2f9a33b752e28304a49'

// First we configure data retrieval.
const processor = new EvmBatchProcessor()
  // SQD Network gateways are the primary source of blockchain data in
  // squids, providing pre-filtered data in chunks of roughly 1-10k blocks.
  // Set this for a fast sync.
  .setGateway('https://v2.archive.subsquid.io/network/ethereum-mainnet')
  // Another data source squid processors can use is chain RPC.
  // In this particular squid it is used to retrieve the very latest chain data
  // (including unfinalized blocks) in real time. It can also be used to
  //   - make direct RPC queries to get extra data during indexing
  //   - sync a squid without a gateway (slow)
  // .setRpcEndpoint('https://rpc.ankr.com/eth')
  .setRpcEndpoint('https://eth.llamarpc.com')
  // The processor needs to know how many newest blocks it should mark as "hot".
  // If it detects a blockchain fork, it will roll back any changes to the
  // database made due to orphaned blocks, then re-run the processing for the
  // main chain blocks.
  .setFinalityConfirmation(75)
  // .addXXX() methods request data items. In this case we're asking for
  // Transfer(address,address,uint256) event logs emitted by the USDC contract.
  //
  // We could have omitted the "address" filter to get Transfer events from
  // all contracts, or the "topic0" filter to get all events from the USDC
  // contract, or both to get all event logs chainwide. We also could have
  // requested some related data, such as the parent transaction or its traces.
  //
  // Other .addXXX() methods (.addTransaction(), .addTrace(), .addStateDiff()
  // on EVM) are similarly feature-rich.
  .addLog({
    // range: { from: 21557766 },
    range: { from: 21557766, to: 21615680 },
    address: [USDC_CONTRACT_ADDRESS],
    topic0: [usdcAbi.events.Transfer.topic],
  })
  // .setFields() is for choosing data fields for the selected data items.
  // Here we're requesting hashes of parent transaction for all event logs.
  .setFields({
    log: {
      transactionHash: true,
    },
  })

// TypeormDatabase objects store the data to Postgres. They are capable of
// handling the rollbacks that occur due to blockchain forks.
//
// There are also Database classes for storing data to files and BigQuery
// datasets.
const db = new TypeormDatabase({ supportHotBlocks: false })
var nextHourlyFlush = 0;
const addressMap = new Map<string, { balance: bigint, windowStartTs: number }>();

type AddressBalance = { address: string, balance: bigint, windowStartTs: number, windowEndTs: number };
const balances: AddressBalance[] = [];
const msPerHour = 3600 * 1000;


// The processor.run() call executes the data processing. Its second argument is
// the handler function that is executed once on each batch of data. Processor
// object provides the data via "ctx.blocks". However, the handler can contain
// arbitrary TypeScript code, so it's OK to bring in extra data from IPFS,
// direct RPC calls, external APIs etc.
processor.run(db, async (ctx) => {
  // Making the container to hold that which will become the rows of the
  // usdc_transfer database table while processing the batch. We'll insert them
  // all at once at the end, massively saving IO bandwidth.
  const transfers: UsdcTransfer[] = []

  // The data retrieved from the SQD Network gatewat and/or the RPC endpoint
  // is supplied via ctx.blocks
  for (let block of ctx.blocks) {

    // On EVM, each block has four iterables - logs, transactions, traces,
    // stateDiffs
    for (let log of block.logs) {
      if (log.address === USDC_CONTRACT_ADDRESS &&
        log.topics[0] === usdcAbi.events.Transfer.topic) {
        // SQD's very own EVM codec at work - about 20 times faster than ethers
        let { from, to, value } = usdcAbi.events.Transfer.decode(log)
        const prevEntryFrom = addressMap.get(from);
        const prevEntryTo = addressMap.get(to);
        const prevBalanceFrom = prevEntryFrom ? prevEntryFrom.balance : 0n;
        const prevWindowStartTsFrom = prevEntryFrom ? prevEntryFrom.windowStartTs : block.header.timestamp;
        const prevBalanceTo = prevEntryTo ? prevEntryTo.balance : 0n;
        const prevWindowStartTsTo = prevEntryTo ? prevEntryTo.windowStartTs : block.header.timestamp;
        // we only emit a balance change if there is a previous entry
        if (prevEntryFrom) {
          balances.push({ address: from, balance: prevBalanceFrom, windowStartTs: prevWindowStartTsFrom, windowEndTs: block.header.timestamp });
        }
        if (prevEntryTo) {
          balances.push({ address: to, balance: prevBalanceTo, windowStartTs: prevWindowStartTsTo, windowEndTs: block.header.timestamp });
        }
        // update the balance in the map with the new balance and window start time
        addressMap.set(to, { balance: prevBalanceTo + value, windowStartTs: block.header.timestamp });
        addressMap.set(from, { balance: prevBalanceFrom - value, windowStartTs: block.header.timestamp });

        transfers.push(new UsdcTransfer({
          id: log.id,
          block: block.header.height,
          from,
          to,
          value,
          txnHash: log.transactionHash
        }))
      }
    }
    // flush balances after the logs
    const currentTs = block.header.timestamp;
    if (currentTs >= nextHourlyFlush) {
      console.log("Hourly flushing at " + currentTs);
      // ... do hourly flush...
      for (let [address, data] of addressMap.entries()) {
        // we only emit a balance change for non-zero balances and if we didn't already just emit a balance change for this address
        if (data.balance > 0 && data.windowStartTs !== currentTs) {
          // console.log('emitting row for ' + address + ' with balance ' + data.balance + ' at ' + currentTs);
          const lastWindowStartTs = addressMap.get(address)!.windowStartTs; // we should always see this since we're iterating over the map
          balances.push({ address, balance: data.balance, windowStartTs: lastWindowStartTs, windowEndTs: currentTs });
          addressMap.set(address, { balance: data.balance, windowStartTs: currentTs });
        }
      }
      // set the next flush to be one hour from current block timestamp
      // nextHourlyFlush = Math.floor(nextHourlyFlush / 3600) * 3600 + 3600;
      nextHourlyFlush = Math.floor(currentTs / msPerHour) * msPerHour + msPerHour;
    }

    if (block.header.height === 21615680) {
      const balancesH = balances.map(d => ({ ...d, start: new Date(d.windowStartTs).toISOString(), end: new Date(d.windowEndTs).toISOString() }));
      fs.writeFileSync('balances.json', JSON.stringify(balancesH, (key, value) =>
        typeof value === 'bigint' ? value.toString() : value
        , 2));
    }
  }

  // Just one insert per batch!
  await ctx.store.insert(transfers)
})
